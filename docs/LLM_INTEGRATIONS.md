# LLM Integration

This application is designed to be flexible in its use of Large Language Models. The core logic for interacting with the LLM is centralized in `backend/llm.py`.

## Default Integration: Ollama

By default, the application uses **Ollama** to serve and interact with local LLMs. This provides an easy and powerful way to run models like Gemma on your local machine.

### How It Works

1.  **Service:** The `docker-compose.yml` file defines an `ollama` service that runs the official `ollama/ollama` image.
2.  **Configuration:** The backend is configured via environment variables in the `.env` file to connect to the Ollama service (`OLLAMA_HOST=http://ollama:11434`).
3.  **Model:** The application will use the model specified by the `MODEL_NAME` environment variable (e.g., `gemma:2b`). When the application starts, you may need to run `docker compose exec ollama ollama pull gemma:2b` to ensure the model is available.
4.  **Interaction:** The backend uses the official `ollama` Python library to send prompts and receive streaming responses from the model. All communication happens over HTTP between the `backend` and `ollama` containers on the internal Docker network.

This setup is ideal for local development and testing as it doesn't require any external API keys and keeps the entire application stack self-contained.

## Alternative Integration: Hugging Face Transformers

While not implemented by default, the architecture allows for swapping out the Ollama client with a different implementation, such as one using the Hugging Face `transformers` library.

To do this, you would primarily need to modify `backend/llm.py`:

1.  **Install Dependencies:** Add `transformers`, `torch`, and `accelerate` to `backend/requirements.txt`.
2.  **Update `stream_ollama_response`:** Replace the `ollama.AsyncClient` logic with code that loads a model and tokenizer from Hugging Face (`AutoModelForCausalLM`, `AutoTokenizer`).
3.  **Implement Streaming:** Use the `TextIteratorStreamer` from the `transformers` library to create a generator that yields tokens as they are generated by the model.
4.  **Configuration:** Update the environment variables in `.env` to manage Hugging Face model names, device placement (CPU/GPU), and other relevant parameters.

The rest of the application (the FastAPI endpoints and the frontend) would not need to change, as they are decoupled from the specific LLM implementation and only depend on the async generator provided by the function.
